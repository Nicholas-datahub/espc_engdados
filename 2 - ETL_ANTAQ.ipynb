{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a833bcaf-c118-46e9-b144-5d1a942eddd5",
   "metadata": {},
   "source": [
    "# OBSERVATÓRIO DA INDÚSTRIA - Prova prática para vaga de Especialista de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cb886-b79d-4f81-acd9-c13b850a4401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T15:01:06.364950Z",
     "iopub.status.busy": "2025-02-22T15:01:06.363938Z",
     "iopub.status.idle": "2025-02-22T15:01:06.369644Z",
     "shell.execute_reply": "2025-02-22T15:01:06.368456Z",
     "shell.execute_reply.started": "2025-02-22T15:01:06.364950Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dea5d7f-bdca-4951-888f-1c426b57d4a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:49:57.798461Z",
     "iopub.status.busy": "2025-02-24T00:49:57.797954Z",
     "iopub.status.idle": "2025-02-24T00:50:00.507414Z",
     "shell.execute_reply": "2025-02-24T00:50:00.506833Z",
     "shell.execute_reply.started": "2025-02-24T00:49:57.798461Z"
    }
   },
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from sql_params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f881ec4-fdba-4853-9c32-3ac33087898d",
   "metadata": {},
   "source": [
    "## Configurações globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49df9fde-766a-4542-ad31-db914c556355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.509467Z",
     "iopub.status.busy": "2025-02-24T00:50:00.508460Z",
     "iopub.status.idle": "2025-02-24T00:50:00.513773Z",
     "shell.execute_reply": "2025-02-24T00:50:00.513228Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.509467Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://web3.antaq.gov.br/ea/txt/\"\n",
    "RAW_DATA_DIR = r\"C:\\data\\antaq\\raw\"\n",
    "JDBC_DRIVER_PATH = r\"C:\\Users\\Fluency Academy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\jars\\mssql-jdbc-12.8.1.jre11.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71d49d-6597-4713-8dda-73b650cfc473",
   "metadata": {},
   "source": [
    "## Tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6ab24-8c61-4428-a3bb-d28f931412bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:13:22.011120Z",
     "iopub.status.busy": "2025-02-24T00:13:22.011120Z",
     "iopub.status.idle": "2025-02-24T00:13:22.014535Z",
     "shell.execute_reply": "2025-02-24T00:13:22.014535Z",
     "shell.execute_reply.started": "2025-02-24T00:13:22.011120Z"
    }
   },
   "source": [
    "### Tarefa: Gerenciamento do banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764ef65b-f20e-4e8a-b7e0-379c0b6e5ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.515544Z",
     "iopub.status.busy": "2025-02-24T00:50:00.515015Z",
     "iopub.status.idle": "2025-02-24T00:50:00.524791Z",
     "shell.execute_reply": "2025-02-24T00:50:00.523781Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.515544Z"
    }
   },
   "outputs": [],
   "source": [
    "@task\n",
    "def create_database(connection_string_master: str):\n",
    "    \"\"\"Cria o banco de dados se não existir\"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string_master)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = 'antaq_db')\n",
    "            BEGIN\n",
    "                CREATE DATABASE antaq_db;\n",
    "            END\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        print(\"[Database] Banco antaq_db verificado/criado com sucesso\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Database] Erro na criação do banco: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def create_sql_tables(connection_string: str):\n",
    "    \"\"\"Cria as tabelas se não existirem\"\"\"\n",
    "    table_definitions = {\n",
    "        \"atracacao_fato\": \"\"\"\n",
    "            IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='atracacao_fato' AND xtype='U')\n",
    "            CREATE TABLE dbo.atracacao_fato (\n",
    "                idatracacao INT PRIMARY KEY,\n",
    "                cdtup VARCHAR(50),\n",
    "                idberco VARCHAR(50),\n",
    "                berço VARCHAR(50),\n",
    "                porto_atracação VARCHAR(100),\n",
    "                coordenadas VARCHAR(100),\n",
    "                apelido_instalação_portuária VARCHAR(100),\n",
    "                complexo_portuário VARCHAR(100),\n",
    "                tipo_da_autoridade_portuária VARCHAR(100),\n",
    "                data_atracação DATE,\n",
    "                data_chegada DATE,\n",
    "                data_desatracação DATE,\n",
    "                data_início_operação DATE,\n",
    "                data_término_operação DATE,\n",
    "                ano INT,\n",
    "                mes VARCHAR(50),\n",
    "                tipo_de_operação VARCHAR(100),\n",
    "                tipo_de_navegação_da_atracação VARCHAR(100),\n",
    "                nacionalidade_do_armador INT,\n",
    "                flagmcoperacaoatracacao INT,\n",
    "                terminal VARCHAR(50),\n",
    "                município VARCHAR(50),\n",
    "                uf VARCHAR(50),\n",
    "                sguf VARCHAR(50),\n",
    "                região_geográfica VARCHAR(100),\n",
    "                região_hidrográfica VARCHAR(100),\n",
    "                instalação_portuária_em_rio VARCHAR(100),\n",
    "                nº_da_capitania VARCHAR(50),\n",
    "                nº_do_imo INT\n",
    "            )\"\"\",\n",
    "        \"carga_fato\": \"\"\"\n",
    "            IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='carga_fato' AND xtype='U')\n",
    "            CREATE TABLE dbo.carga_fato (\n",
    "                idcarga INT PRIMARY KEY,\n",
    "                idatracacao INT,\n",
    "                origem VARCHAR(100),\n",
    "                destino VARCHAR(100),\n",
    "                cdmercadoria VARCHAR(50),\n",
    "                tipo_operação_da_carga VARCHAR(100),\n",
    "                carga_geral_acondicionamento VARCHAR(100),\n",
    "                conteinerestado VARCHAR(50),\n",
    "                tipo_navegação VARCHAR(100),\n",
    "                flagautorizacao INT,\n",
    "                flagcabotagem INT,\n",
    "                flagcabotagemmovimentacao INT,\n",
    "                flagconteinertamanho INT,\n",
    "                flaglongocurso INT,\n",
    "                flagmcoperacaocarga INT,\n",
    "                flagoffshore INT,\n",
    "                flagtransporteviainterioir INT,\n",
    "                percurso_transporte_em_vias_interiores INT,\n",
    "                percurso_transporte_interiores INT,\n",
    "                stnaturezacarga VARCHAR(100),\n",
    "                stsh2 VARCHAR(50),\n",
    "                stsh4 VARCHAR(50),\n",
    "                natureza_da_carga VARCHAR(100),\n",
    "                sentido VARCHAR(50),\n",
    "                teu INT,\n",
    "                qtcarga INT,\n",
    "                vlpesocargabruta FLOAT\n",
    "            )\"\"\"\n",
    "    }\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        for table_name, ddl in table_definitions.items():\n",
    "            cursor.execute(f\"\"\"\n",
    "                IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{table_name}' AND xtype='U')\n",
    "                BEGIN\n",
    "                    {ddl}\n",
    "                END\n",
    "            \"\"\")\n",
    "            print(f\"[Database] Tabela {table_name} verificada/criada\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Database] Erro na criação das tabelas: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1695e-e394-4fcf-a22e-a6a6dad8927c",
   "metadata": {},
   "source": [
    "### Tarefa: Coleta de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d217d41-c3f2-455f-a8e1-823a3fbcc6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.525789Z",
     "iopub.status.busy": "2025-02-24T00:50:00.525789Z",
     "iopub.status.idle": "2025-02-24T00:50:00.534815Z",
     "shell.execute_reply": "2025-02-24T00:50:00.533808Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.525789Z"
    }
   },
   "outputs": [],
   "source": [
    "@task\n",
    "def download_and_process_month(base_url: str, year: int, data_type: str):\n",
    "    \"\"\"Baixa e extrai dados para um ano e tipo específico\"\"\"\n",
    "    try:\n",
    "        url = f\"{base_url}{year}{data_type}.zip\"\n",
    "        output_dir = os.path.join(RAW_DATA_DIR, str(year), data_type.lower())\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(f\"[Raw Data] Criado diretório: {output_dir}\")\n",
    "\n",
    "        # Verifica conteúdo antes do download\n",
    "        existing_files = os.listdir(output_dir)\n",
    "        if len(existing_files) > 0:\n",
    "            print(f\"[Raw Data] Arquivos existentes ({len(existing_files)}):\")\n",
    "            for f in existing_files:\n",
    "                file_path = os.path.join(output_dir, f)\n",
    "                print(f\" - {f} ({os.path.getsize(file_path)/1024:.2f} KB)\")\n",
    "            \n",
    "        if len(os.listdir(output_dir)) == 0:\n",
    "            print(f\"[Download] Iniciando download de {url}\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                # Verifica conteúdo do ZIP antes de extrair\n",
    "                zip_info = z.infolist()\n",
    "                print(f\"[Download] Arquivos no ZIP ({len(zip_info)}):\")\n",
    "                for info in zip_info:\n",
    "                    print(f\" - {info.filename} ({info.file_size/1024:.2f} KB)\")\n",
    "                \n",
    "                z.extractall(output_dir)\n",
    "                print(f\"[Download] Dados extraídos em: {output_dir}\")\n",
    "\n",
    "                # Verifica arquivos extraídos\n",
    "                extracted_files = os.listdir(output_dir)\n",
    "                print(f\"[Raw Data] Arquivos extraídos ({len(extracted_files)}):\")\n",
    "                for f in extracted_files:\n",
    "                    file_path = os.path.join(output_dir, f)\n",
    "                    print(f\" - {f} ({os.path.getsize(file_path)/1024:.2f} KB)\")\n",
    "        else:\n",
    "            print(f\"[Raw Data] Dados já existem em: {output_dir}\")\n",
    "            \n",
    "        return output_dir\n",
    "    except Exception as e:\n",
    "        print(f\"[Download] Falha no processamento de {data_type} {year}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a8638-3397-47c0-af85-49c5cee98b65",
   "metadata": {},
   "source": [
    "### Tarefa: Processamento Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4563831a-98ff-4538-a285-c3134acf9701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.535814Z",
     "iopub.status.busy": "2025-02-24T00:50:00.534815Z",
     "iopub.status.idle": "2025-02-24T00:50:00.554445Z",
     "shell.execute_reply": "2025-02-24T00:50:00.553940Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.535814Z"
    }
   },
   "outputs": [],
   "source": [
    "@task\n",
    "def process_and_load_data(atracacao_dir: str, carga_dir: str, connection_properties: dict):\n",
    "    \"\"\"Processa e carrega dados no SQL Server\"\"\"\n",
    "    try:\n",
    "        # Iniciar a sessão Spark\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ETL_ANTAQ\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\") \\\n",
    "            .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "            .config(\"spark.hadoop.io.nativeio.NativeIO\", \"false\") \\\n",
    "            .config(\"spark.hadoop.io.nativeio.NativeIO.disable\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        # ========== Processamento Atracacao ==========\n",
    "        print(f\"\\n[Spark] Verificando arquivos em: {atracacao_dir}\")\n",
    "        atracacao_files = [f for f in os.listdir(atracacao_dir) if f.endswith('.txt')]\n",
    "        if not atracacao_files:\n",
    "            print(f\"[Spark] Nenhum arquivo TXT encontrado em: {atracacao_dir}\")\n",
    "            return\n",
    "\n",
    "        atracacao_path = os.path.join(atracacao_dir, atracacao_files[0])\n",
    "        print(f\"[Spark] Processando arquivo de atracacao: {atracacao_path}\")\n",
    "\n",
    "        df_atracacao = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"delimiter\", \";\") \\\n",
    "            .csv(atracacao_path)\n",
    "        \n",
    "        print(f\"[Spark] Total de registros brutos de atracacao: {df_atracacao.count():,}\")\n",
    "    \n",
    "        # Dicionário com as colunas a serem utilizadas para atracação\n",
    "        atracacao_columns = [\n",
    "            'IDAtracacao', 'CDTUP', 'IDBerco', 'Berço', 'Porto Atracação', 'Coordenadas',\n",
    "            'Apelido Instalação Portuária', 'Complexo Portuário', 'Tipo da Autoridade Portuária',\n",
    "            'Data Atracação', 'Data Chegada', 'Data Desatracação', 'Data Início Operação',\n",
    "            'Data Término Operação', 'Ano', 'Mes', 'Tipo de Operação', 'Tipo de Navegação da Atracação',\n",
    "            'Nacionalidade do Armador', 'FlagMCOperacaoAtracacao', 'Terminal', 'Município', 'UF',\n",
    "            'SGUF', 'Região Geográfica', 'Região Hidrográfica', 'Instalação Portuária em Rio',\n",
    "            'Nº da Capitania', 'Nº do IMO'\n",
    "        ]\n",
    "    \n",
    "        # Normalizando nomes de colunas, substituindo espaços por _\n",
    "        atracacao_mapping = {name: name.lower().replace(\" \", \"_\") for name in atracacao_columns}\n",
    "        \n",
    "        df_atracacao_fato = df_atracacao.select([col(c).alias(atracacao_mapping[c]) for c in atracacao_mapping]) \\\n",
    "                                    .dropDuplicates([atracacao_mapping.get(\"idatracacao\", \"IDatracacao\")])\n",
    "\n",
    "        # Escrevendo no Data Lake (Parquet)\n",
    "        atracacao_output = \"/data/lake/antaq/atracacao_fato/\"\n",
    "        df_atracacao_fato.write.mode(\"overwrite\").parquet(atracacao_output)\n",
    "        print(f\"[Data Lake] Atracação salva em: {atracacao_output}\")\n",
    "    \n",
    "        # ==================== Processamento de dados de Carga ====================\n",
    "        df_carga_raw = spark.read.option(\"header\", \"true\") \\\n",
    "                                 .option(\"inferSchema\", \"true\") \\\n",
    "                                 .option(\"delimiter\", \";\") \\\n",
    "                                 .csv(carga_dir)\n",
    "        print(f\"Total de registros de carga_raw: {df_carga_raw.count()}\")\n",
    "    \n",
    "        df_atracacao_join = df_atracacao_fato.select(col(atracacao_mapping.get(\"idatracacao\", \"idatracacao\")).alias(\"join_id\")).distinct()\n",
    "\n",
    "        # Escreve no SQL Server\n",
    "        print(\"\\n[Spark] Escrevendo dados de atracação no banco...\")\n",
    "        df_atracacao_fato.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName=antaq_db;trustServerCertificate=true\") \\\n",
    "            .option(\"dbtable\", \"dbo.atracacao_fato\") \\\n",
    "            .option(\"user\", connection_properties[\"user\"]) \\\n",
    "            .option(\"password\", connection_properties[\"password\"]) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"[SQL Server] Atracação concluída - Esperados {df_atracacao_fato.count():,} registros\")\n",
    "        \n",
    "        # Usando join com a tabela Atracação para filtrar apenas os dados que constam na primeira tabela\n",
    "        df_carga = df_carga_raw.join(\n",
    "            df_atracacao_join,\n",
    "            df_carga_raw[\"idatracacao\"] == df_atracacao_join[\"join_id\"],\n",
    "            \"inner\"\n",
    "        ).drop(\"join_id\")\n",
    "        \n",
    "        carga_columns = [\n",
    "            'IDCarga', 'IDAtracacao', 'Origem', 'Destino', 'CDMercadoria', 'Tipo Operação da Carga',\n",
    "            'Carga Geral Acondicionamento', 'ConteinerEstado', 'Tipo Navegação', 'FlagAutorizacao',\n",
    "            'FlagCabotagem', 'FlagCabotagemMovimentacao', 'FlagConteinerTamanho', 'FlagLongoCurso',\n",
    "            'FlagMCOperacaoCarga', 'FlagOffshore', 'FlagTransporteViaInterioir', 'Percurso Transporte em vias Interiores',\n",
    "            'Percurso Transporte Interiores', 'STNaturezaCarga', 'STSH2', 'STSH4', 'Natureza da Carga', 'Sentido',\n",
    "            'TEU', 'QTCarga', 'VLPesoCargaBruta'\n",
    "        ]\n",
    "    \n",
    "        # Normalizando nomes de colunas, substituindo espaços por _\n",
    "        carga_mapping = {name: name.lower().replace(\" \", \"_\") for name in carga_columns if name in df_carga.columns}\n",
    "    \n",
    "        df_carga_fato = df_carga.select([col(c).alias(carga_mapping[c]) for c in carga_mapping]) \\\n",
    "                                .dropDuplicates([carga_mapping.get(\"idcarga\", \"idcarga\")])\n",
    "\n",
    "        # Escrevendo no Data Lake (Parquet)\n",
    "        carga_output = \"/data/lake/antaq/carga_fato/\"\n",
    "        df_carga_fato.write.mode(\"overwrite\").parquet(carga_output)\n",
    "        print(f\"[Data Lake] Carga salva em: {carga_output}\")\n",
    "\n",
    "        # Escreve no SQL Server\n",
    "        print(\"\\n[Spark] Escrevendo dados de carga no banco...\")\n",
    "        df_carga_fato.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName=antaq_db;trustServerCertificate=true\") \\\n",
    "            .option(\"dbtable\", \"dbo.carga_fato\") \\\n",
    "            .option(\"user\", connection_properties[\"user\"]) \\\n",
    "            .option(\"password\", connection_properties[\"password\"]) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"[SQL Server] Carga concluída - Esperados {df_carga_fato.count():,} registros\")\n",
    "\n",
    "        spark.stop()\n",
    "        \n",
    "        # Verificação final\n",
    "        print(\"\\n[Verificação] Verificando registros no banco...\")\n",
    "        try:\n",
    "            conn = pyodbc.connect(connection_properties[\"connection_string\"])\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"SELECT COUNT(*) FROM atracacao_fato\")\n",
    "            atracacao_count = cursor.fetchone()[0]\n",
    "            print(f\"[SQL] Total de registros em atracacao_fato: {atracacao_count:,}\")\n",
    "            \n",
    "            cursor.execute(\"SELECT COUNT(*) FROM carga_fato\")\n",
    "            carga_count = cursor.fetchone()[0]\n",
    "            print(f\"[SQL] Total de registros em carga_fato: {carga_count:,}\")\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            if atracacao_count < df_atracacao_fato.count():\n",
    "                print(f\"[AVISO] Possível perda de dados: Esperados {df_atracacao_fato.count():,} vs Inseridos {atracacao_count:,}\")\n",
    "                \n",
    "            if carga_count < df_carga_fato.count():\n",
    "                print(f\"[AVISO] Possível perda de dados: Esperados {df_carga_fato.count():,} vs Inseridos {carga_count:,}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha na verificação do banco: {str(e)}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[Spark] Erro no processamento: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab4b48-b85a-4196-83f7-62ed54aa6f69",
   "metadata": {},
   "source": [
    "## Fluxo Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11287cfe-3738-466e-8388-29eae1421ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.556456Z",
     "iopub.status.busy": "2025-02-24T00:50:00.556456Z",
     "iopub.status.idle": "2025-02-24T00:50:00.569806Z",
     "shell.execute_reply": "2025-02-24T00:50:00.569274Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.556456Z"
    }
   },
   "outputs": [],
   "source": [
    "@flow\n",
    "def antaq_etl_pipeline():\n",
    "    \"\"\"Orquestração principal do pipeline ETL\"\"\"\n",
    "    try:\n",
    "        conn_master = f\"DRIVER={{{driver}}};SERVER={server};DATABASE=master;UID={user};PWD={password};TrustServerCertificate=yes\"\n",
    "        conn_antaq = f\"DRIVER={{{driver}}};SERVER={server};DATABASE=antaq_db;UID={user};PWD={password};TrustServerCertificate=yes\"\n",
    "        \n",
    "        create_database(conn_master)\n",
    "        create_sql_tables(conn_antaq)\n",
    "        \n",
    "        # Processar cada ano especificado\n",
    "        for current_year in [2021, 2022, 2023]:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processando ano: {year}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            atracacao_dir = download_and_process_month(BASE_URL, current_year, \"Atracacao\")\n",
    "            carga_dir = download_and_process_month(BASE_URL, current_year, \"Carga\")\n",
    "            \n",
    "            process_and_load_data(\n",
    "                atracacao_dir,\n",
    "                carga_dir,\n",
    "                {\n",
    "                    \"user\": user,\n",
    "                    \"password\": password,\n",
    "                    \"driver\": driver\n",
    "                }\n",
    "            )\n",
    "        data_lake_path = \"data/lake/antaq\"\n",
    "\n",
    "        df_atracacao_fato.write.mode(\"overwrite\").parquet(f\"{data_lake_path}/atracacao_fato/\")\n",
    "        df_carga_fato.write.mode(\"overwrite\").parquet(f\"{data_lake_path}/carga_fato/\")\n",
    "\n",
    "        print(\"[Pipeline] Processamento concluído para todos os anos\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Pipeline] Erro fatal no pipeline: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c47de-8f3f-4c1b-a95f-1dee07fbfc58",
   "metadata": {},
   "source": [
    "## Execução e início de orquestração com Prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb4f010-2631-4d8c-bddf-72659dba8661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T00:50:00.571804Z",
     "iopub.status.busy": "2025-02-24T00:50:00.571254Z",
     "iopub.status.idle": "2025-02-24T00:56:18.843405Z",
     "shell.execute_reply": "2025-02-24T00:56:18.839537Z",
     "shell.execute_reply.started": "2025-02-24T00:50:00.571804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:50:03.629 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8945</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:50:03.629 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8945\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database] Banco antaq_db verificado/criado com sucesso\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:50:10.988 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'create_database' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:50:10.988 | \u001b[36mINFO\u001b[0m    | Task run 'create_database' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database] Tabela atracacao_fato verificada/criada\n",
      "[Database] Tabela carga_fato verificada/criada\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:50:11.962 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'create_sql_tables' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:50:11.962 | \u001b[36mINFO\u001b[0m    | Task run 'create_sql_tables' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "PROCESSANDO ANO 2021\n",
      "==============================\n",
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2021Atracacao.txt (26464.83 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2021\\atracacao\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:50:12.521 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:50:12.521 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2021Carga.txt (428676.96 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2021\\carga\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:50:13.085 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:50:13.085 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Spark] Verificando arquivos em: C:\\data\\antaq\\raw\\2021\\atracacao\n",
      "[Spark] Processando arquivo de atracacao: C:\\data\\antaq\\raw\\2021\\atracacao\\2021Atracacao.txt\n",
      "[Spark] Total de registros brutos de atracacao: 79,238\n",
      "[Data Lake] Atracação salva em: /data/lake/antaq/atracacao_fato/\n",
      "Total de registros de carga_raw: 2348365\n",
      "\n",
      "[Spark] Escrevendo dados de atracação no banco...\n",
      "[SQL Server] Atracação concluída - Esperados 79,238 registros\n",
      "[Data Lake] Carga salva em: /data/lake/antaq/carga_fato/\n",
      "\n",
      "[Spark] Escrevendo dados de carga no banco...\n",
      "[SQL Server] Carga concluída - Esperados 2,348,365 registros\n",
      "\n",
      "[Verificação] Verificando registros no banco...\n",
      "[ERRO] Falha na verificação do banco: 'connection_string'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:52:35.687 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'process_and_load_data' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:52:35.687 | \u001b[36mINFO\u001b[0m    | Task run 'process_and_load_data' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "PROCESSANDO ANO 2022\n",
      "==============================\n",
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2022Atracacao.txt (28470.70 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2022\\atracacao\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:52:36.362 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:52:36.362 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2022Carga.txt (414565.53 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2022\\carga\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:52:36.922 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:52:36.922 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Spark] Verificando arquivos em: C:\\data\\antaq\\raw\\2022\\atracacao\n",
      "[Spark] Processando arquivo de atracacao: C:\\data\\antaq\\raw\\2022\\atracacao\\2022Atracacao.txt\n",
      "[Spark] Total de registros brutos de atracacao: 85,092\n",
      "[Data Lake] Atracação salva em: /data/lake/antaq/atracacao_fato/\n",
      "Total de registros de carga_raw: 2280018\n",
      "\n",
      "[Spark] Escrevendo dados de atracação no banco...\n",
      "[SQL Server] Atracação concluída - Esperados 85,092 registros\n",
      "[Data Lake] Carga salva em: /data/lake/antaq/carga_fato/\n",
      "\n",
      "[Spark] Escrevendo dados de carga no banco...\n",
      "[SQL Server] Carga concluída - Esperados 2,280,018 registros\n",
      "\n",
      "[Verificação] Verificando registros no banco...\n",
      "[ERRO] Falha na verificação do banco: 'connection_string'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:54:27.282 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'process_and_load_data' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:54:27.282 | \u001b[36mINFO\u001b[0m    | Task run 'process_and_load_data' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "PROCESSANDO ANO 2023\n",
      "==============================\n",
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2023Atracacao.txt (31590.45 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2023\\atracacao\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:54:27.951 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:54:27.951 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Data] Arquivos existentes (1):\n",
      " - 2023Carga.txt (395917.38 KB)\n",
      "[Raw Data] Dados já existem em: C:\\data\\antaq\\raw\\2023\\carga\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:54:28.544 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'download_and_process_month' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:54:28.544 | \u001b[36mINFO\u001b[0m    | Task run 'download_and_process_month' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Spark] Verificando arquivos em: C:\\data\\antaq\\raw\\2023\\atracacao\n",
      "[Spark] Processando arquivo de atracacao: C:\\data\\antaq\\raw\\2023\\atracacao\\2023Atracacao.txt\n",
      "[Spark] Total de registros brutos de atracacao: 93,908\n",
      "[Data Lake] Atracação salva em: /data/lake/antaq/atracacao_fato/\n",
      "Total de registros de carga_raw: 2200634\n",
      "\n",
      "[Spark] Escrevendo dados de atracação no banco...\n",
      "[SQL Server] Atracação concluída - Esperados 93,908 registros\n",
      "[Data Lake] Carga salva em: /data/lake/antaq/carga_fato/\n",
      "\n",
      "[Spark] Escrevendo dados de carga no banco...\n",
      "[SQL Server] Carga concluída - Esperados 2,200,634 registros\n",
      "\n",
      "[Verificação] Verificando registros no banco...\n",
      "[ERRO] Falha na verificação do banco: 'connection_string'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">21:56:18.802 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'process_and_load_data' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "21:56:18.802 | \u001b[36mINFO\u001b[0m    | Task run 'process_and_load_data' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESSAMENTO CONCLUÍDO\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Construir connection strings usando parâmetros importados\n",
    "    conn_master = f\"DRIVER={{{driver}}};SERVER={server};DATABASE=master;UID={user};PWD={password};TrustServerCertificate=yes\"\n",
    "    conn_antaq = f\"DRIVER={{{driver}}};SERVER={server};DATABASE=antaq_db;UID={user};PWD={password};TrustServerCertificate=yes\"\n",
    "    \n",
    "    # Executar fora do Prefect (chamada direta das funções)\n",
    "    create_database(conn_master)\n",
    "    create_sql_tables(conn_antaq)\n",
    "    \n",
    "    for year in [2021, 2022, 2023]:\n",
    "        print(f\"\\n{'='*30}\")\n",
    "        print(f\"PROCESSANDO ANO {year}\")\n",
    "        print(f\"{'='*30}\")\n",
    "        \n",
    "        # Baixar dados\n",
    "        atracacao_dir = download_and_process_month(BASE_URL, year, \"Atracacao\")\n",
    "        carga_dir = download_and_process_month(BASE_URL, year, \"Carga\")\n",
    "        \n",
    "        # Processar e carregar\n",
    "        process_and_load_data(\n",
    "            atracacao_dir,\n",
    "            carga_dir,\n",
    "            {\n",
    "                \"user\": user,\n",
    "                \"password\": password,\n",
    "                \"driver\": driver\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPROCESSAMENTO CONCLUÍDO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
